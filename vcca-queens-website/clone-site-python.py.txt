import os
import re
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse

# ==== CONFIG ====
BASE_URL = "http://antiquechevyclubofqueens.org/"
OUTPUT_DIR = "site_clone"
DOWNLOADED = set()

# Ensure output folder exists
os.makedirs(OUTPUT_DIR, exist_ok=True)

def download_file(url):
    """Download a single file and save it to OUTPUT_DIR with same structure."""
    if url in DOWNLOADED:
        return

    parsed = urlparse(url)
    if not parsed.netloc or parsed.netloc not in BASE_URL:
        return  # skip external links

    path = parsed.path
    if path.endswith("/"):
        path += "index.html"

    save_path = os.path.join(OUTPUT_DIR, path.lstrip("/"))
    os.makedirs(os.path.dirname(save_path), exist_ok=True)

    print(f"Downloading: {url}")
    try:
        r = requests.get(url, timeout=10)
        r.raise_for_status()
    except requests.RequestException:
        print(f"❌ Failed: {url}")
        return

    content_type = r.headers.get("content-type", "")
    if "text/html" in content_type:
        soup = BeautifulSoup(r.text, "html.parser")
        fix_and_queue_links(soup, url)
        with open(save_path, "w", encoding="utf-8") as f:
            f.write(str(soup))
    else:
        with open(save_path, "wb") as f:
            f.write(r.content)

    DOWNLOADED.add(url)

def fix_and_queue_links(soup, base):
    """Fix links to be relative & queue new files for download."""
    tags_attrs = {
        "a": "href",
        "img": "src",
        "link": "href",
        "script": "src"
    }
    for tag, attr in tags_attrs.items():
        for el in soup.find_all(tag):
            if el.has_attr(attr):
                abs_url = urljoin(base, el[attr])
                parsed = urlparse(abs_url)
                if parsed.netloc == urlparse(BASE_URL).netloc:
                    # Make relative
                    rel_path = parsed.path.lstrip("/")
                    if not rel_path:
                        rel_path = "index.html"
                    el[attr] = "/" + rel_path
                    # Queue for download
                    download_file(abs_url)

# ==== Start cloning ====
download_file(BASE_URL)
print("\n✅ Done! Site saved in", OUTPUT_DIR)
print("Upload all files inside", OUTPUT_DIR, "to Neocities.")
